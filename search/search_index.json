{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ACKNOWLEDGEMENTS Special thanks to Craig Brandt and other colleagues from the Red Hat team for preparing the sample Ansible Playbooks for the Technical Preview of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed . Generally speaking, Generative AI is the talk of the town in 2023 \u2014 and likely will remain so for years to come. What began to simmer in the early 2020s as excitement within academic and technology community around around transformers and language models has today exploded into a firestorm of interest and investment across nearly every industry, institution, and government. For the first time at scale, everyday consumers have access to artificial intelligence on their phones and through their web browsers. Likewise, enterprise and business leaders no longer view AI as a topic of interest, but as a critical imperative to success in the future economy. Foundation Models \u2014 a term coined by Stanford University \u2014are built using a specific kind of neural network architecture, called a Transformer. The Transformer helps the Foundation Model understand unlabeled data and turn an input into an output. The most consequential manifestation of this technology so far is what we know today as \"Generative AI.\" So what is Generative AI ? The vast topic refers to a subset of AI techniques and methodologies that are designed to generate new content \u2014 in other words, AI applied towards the creation of novel contents in the form of images, text, voice, or even code. This sharply differs from the goals of more \"traditional\" AI models of the past, which have primarily been focused on the analysis and classification (labeling) of information. Historically, these early pioneering techniques of AI models have progressed from simple probabilistic models into increasingly sophisticated systems, building upon concepts like neural networks and deep learning techniques. Today, technologies like Generative AI showcase not only the ability to analysis, but tremendous capacity for creation. Even the creation of application code and automation tasks. The advent of Foundation Models and Generative AI, given their remarkable performance and extensibility to a wide range of tasks, is bringing an inflection point in AI. Recognizing the significance of the moment, IBM enterprise clients are actively evaluating and seeking to incorporate foundation models into critical business workflows for applications involving generation, summarization, classification, and so many other use cases. The notion of automating the generation of Ansible Playbook code with AI stems from the challenges and bottlenecks often faced by developers tasked with traditional, manual creation of Playbooks. These individuals must craft precise, error-free Playbooks which are potentially automation jobs across vast collections of assets or hardware. One of the benefits of automation is being able to perform such tasks at-scale; conversely, this also poses one of the greatest risks of automation \u2014 that when things fail, they can fail rapidly and across vast swathes of IT estate. It should come as no surprise, then, that authoring Playbooks often demands technical expertise and a deep understanding of the targeted systems and services which Ansible is to automate. If you recollect the founding philosophies of Red Hat Ansible\u2014 human-readable automation jobs, the democratization of automation to non-technical audiences \u2014you might think the practicalities of manually creating Ansible Playbooks is antithetical to Ansible's potential. And you would be correct in that logic. Generative AI, especially models like GPT, have recently demonstrated proficiency in creating syntactically correct and contextually relevant application code in a variety of programming languages. If trained on a large dataset of Ansible Playbooks, Generative AI models can be fine-tuned to understand the nuances of Playbook syntax and structure. An enterprise organization with dozens or hundreds of Playbooks within their IT estate today would have a rich corpus of training data on-hand that could be used to fine-tune AI models that are tailored to the automation needs and programming style/standards of that particular company. As we will see later, productized versions of the Generative AI models can provide a natural language prompt or a specific description of the desired task to users \u2014 who in turn can utilize these models for generating the necessary Playbook code. For example, a user might describe a desired system state in plain language (\"I want a Playbook to install and start the Apache web server\") and the model would generate the appropriate Ansible tasks for a Playbook. All of this achieved without physically writing code or requiring much programming expertise. Not only does this speed up the automation process by cutting the time needed to author Playbooks, but it also democratizes access to automation in general. Even those within the company with limited Ansible or programming expertise will be able produce effective Playbooks. There are plenty of caveats of course, and thorough validation and testing will be needed to safeguard against the at-scale automation disaster scenarios that were mentioned previously. However, the productivity gains and broadening of skillsets within an organization can be tremendous. And as a whole, Generative AI brings the original goals of Red Hat Ansible Automation Platform (the democratization of automation for everything) that much closer to a reality. The material covered in this Level 3 coursework will prepare IBM and business partner technical sellers with the skills necessary to demonstrate Ansible Playbook task creation using the generative AI capabilities of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed . A client demo-ready Technical Preview is available free of charge through the open source community, which you will make use of as well for this hands-on learning via extensions in VS Code (Visual Studio Code) on your local machine. This service uses, among other data, roles and collections that are available through the Red Hat community website, Ansible Galaxy . The documentation within this Level 3 lab will cover how to use set up Code VS on a local machine (macOS or Windows) with an extension for IBM watsonx Code Assistant for Red Hat Ansible Lightspeed . You will then leverage the generative AI content recommendations for creating a variety of Ansible Playbooks for automating cloud-based and infrastructure-based automation tasks, such as: Install and configure Cockpit for Ansible Preparing an AWS and Azure cloud environment Provisioning an AWS EC2 instance Provisioning an Azure virtual machine (VM) Run a Podman 'pgadmin' container Deploy and start a PostgreSQL database server These short demonstrations will go beyond simply giving you hands-on experience with Ansible Lightspeed's generative AI capabilities for Ansible Playbook task creation. In-depth explanations accomanying the Playbooks will also explain: The integrations that Ansible Lightspeed and IBM watsonx provide in terms of content tooling How Ansible Lightspeed natural language prompts, as well as the Ansible Playbook contents, to create contextually-aware code suggestions for Ansible Tasks Pre-processing and post-processing capabilities that refine the generative AI suggestions into syntactically correct code (adherent to best practices) How Ansible Lightspeed provides \"explainability\" and source content matching attribution for all AI-generated content Ahead, we will cover the pre-requisites for getting started with the hands-on material, as well as outline the evaluation criteria for IBM technical sellers and business partners.","title":"Introduction"},{"location":"#_1","text":"","title":""},{"location":"#generally-speaking-generative-ai-is-the-talk-of-the-town-in-2023-and-likely-will-remain-so-for-years-to-come","text":"What began to simmer in the early 2020s as excitement within academic and technology community around around transformers and language models has today exploded into a firestorm of interest and investment across nearly every industry, institution, and government. For the first time at scale, everyday consumers have access to artificial intelligence on their phones and through their web browsers. Likewise, enterprise and business leaders no longer view AI as a topic of interest, but as a critical imperative to success in the future economy. Foundation Models \u2014 a term coined by Stanford University \u2014are built using a specific kind of neural network architecture, called a Transformer. The Transformer helps the Foundation Model understand unlabeled data and turn an input into an output. The most consequential manifestation of this technology so far is what we know today as \"Generative AI.\" So what is Generative AI ? The vast topic refers to a subset of AI techniques and methodologies that are designed to generate new content \u2014 in other words, AI applied towards the creation of novel contents in the form of images, text, voice, or even code. This sharply differs from the goals of more \"traditional\" AI models of the past, which have primarily been focused on the analysis and classification (labeling) of information. Historically, these early pioneering techniques of AI models have progressed from simple probabilistic models into increasingly sophisticated systems, building upon concepts like neural networks and deep learning techniques. Today, technologies like Generative AI showcase not only the ability to analysis, but tremendous capacity for creation. Even the creation of application code and automation tasks. The advent of Foundation Models and Generative AI, given their remarkable performance and extensibility to a wide range of tasks, is bringing an inflection point in AI. Recognizing the significance of the moment, IBM enterprise clients are actively evaluating and seeking to incorporate foundation models into critical business workflows for applications involving generation, summarization, classification, and so many other use cases.","title":"Generally speaking, Generative AI is the talk of the town in 2023 \u2014 and likely will remain so for years to come."},{"location":"#_2","text":"","title":""},{"location":"#the-notion-of-automating-the-generation-of-ansible-playbook-code-with-ai-stems-from-the-challenges-and-bottlenecks-often-faced-by-developers-tasked-with-traditional-manual-creation-of-playbooks","text":"These individuals must craft precise, error-free Playbooks which are potentially automation jobs across vast collections of assets or hardware. One of the benefits of automation is being able to perform such tasks at-scale; conversely, this also poses one of the greatest risks of automation \u2014 that when things fail, they can fail rapidly and across vast swathes of IT estate. It should come as no surprise, then, that authoring Playbooks often demands technical expertise and a deep understanding of the targeted systems and services which Ansible is to automate. If you recollect the founding philosophies of Red Hat Ansible\u2014 human-readable automation jobs, the democratization of automation to non-technical audiences \u2014you might think the practicalities of manually creating Ansible Playbooks is antithetical to Ansible's potential. And you would be correct in that logic. Generative AI, especially models like GPT, have recently demonstrated proficiency in creating syntactically correct and contextually relevant application code in a variety of programming languages. If trained on a large dataset of Ansible Playbooks, Generative AI models can be fine-tuned to understand the nuances of Playbook syntax and structure. An enterprise organization with dozens or hundreds of Playbooks within their IT estate today would have a rich corpus of training data on-hand that could be used to fine-tune AI models that are tailored to the automation needs and programming style/standards of that particular company. As we will see later, productized versions of the Generative AI models can provide a natural language prompt or a specific description of the desired task to users \u2014 who in turn can utilize these models for generating the necessary Playbook code. For example, a user might describe a desired system state in plain language (\"I want a Playbook to install and start the Apache web server\") and the model would generate the appropriate Ansible tasks for a Playbook. All of this achieved without physically writing code or requiring much programming expertise. Not only does this speed up the automation process by cutting the time needed to author Playbooks, but it also democratizes access to automation in general. Even those within the company with limited Ansible or programming expertise will be able produce effective Playbooks. There are plenty of caveats of course, and thorough validation and testing will be needed to safeguard against the at-scale automation disaster scenarios that were mentioned previously. However, the productivity gains and broadening of skillsets within an organization can be tremendous. And as a whole, Generative AI brings the original goals of Red Hat Ansible Automation Platform (the democratization of automation for everything) that much closer to a reality.","title":"The notion of automating the generation of Ansible Playbook code with AI stems from the challenges and bottlenecks often faced by developers tasked with traditional, manual creation of Playbooks."},{"location":"#_3","text":"","title":""},{"location":"#the-material-covered-in-this-level-3-coursework-will-prepare-ibm-and-business-partner-technical-sellers-with-the-skills-necessary-to-demonstrate-ansible-playbook-task-creation-using-the-generative-ai-capabilities-of-ibm-watsonx-code-assistant-for-red-hat-ansible-lightspeed","text":"A client demo-ready Technical Preview is available free of charge through the open source community, which you will make use of as well for this hands-on learning via extensions in VS Code (Visual Studio Code) on your local machine. This service uses, among other data, roles and collections that are available through the Red Hat community website, Ansible Galaxy . The documentation within this Level 3 lab will cover how to use set up Code VS on a local machine (macOS or Windows) with an extension for IBM watsonx Code Assistant for Red Hat Ansible Lightspeed . You will then leverage the generative AI content recommendations for creating a variety of Ansible Playbooks for automating cloud-based and infrastructure-based automation tasks, such as: Install and configure Cockpit for Ansible Preparing an AWS and Azure cloud environment Provisioning an AWS EC2 instance Provisioning an Azure virtual machine (VM) Run a Podman 'pgadmin' container Deploy and start a PostgreSQL database server These short demonstrations will go beyond simply giving you hands-on experience with Ansible Lightspeed's generative AI capabilities for Ansible Playbook task creation. In-depth explanations accomanying the Playbooks will also explain: The integrations that Ansible Lightspeed and IBM watsonx provide in terms of content tooling How Ansible Lightspeed natural language prompts, as well as the Ansible Playbook contents, to create contextually-aware code suggestions for Ansible Tasks Pre-processing and post-processing capabilities that refine the generative AI suggestions into syntactically correct code (adherent to best practices) How Ansible Lightspeed provides \"explainability\" and source content matching attribution for all AI-generated content Ahead, we will cover the pre-requisites for getting started with the hands-on material, as well as outline the evaluation criteria for IBM technical sellers and business partners.","title":"The material covered in this Level 3 coursework will prepare IBM and business partner technical sellers with the skills necessary to demonstrate Ansible Playbook task creation using the generative AI capabilities of IBM watsonx Code Assistant for Red Hat Ansible Lightspeed."},{"location":"Part%201/01-Prerequisites/","text":"Prerequisites and Setup Before getting started, you must install Visual Studio Code (commonly referred to as VS Code ) on your local machine. VS Code is a code editor built on open source technology, free to use, and published by Microsoft. Distributions are available for every major operating system, including Windows and macOS. Download : https://code.visualstudio.com Select the latest \"stable\" release availabe for your machine's operating system. Follow along with the installer wizard steps and continue after VS Code is successfully running on your local machine. Open the VS Code app and locate the sidebar along the left-side of the interface. Click the Extensions icon as shown to open the marketplace of Microsoft services and open source technologies that can be integrated with VS Code. At the top of the Extensions panel is a search bar. Search here for Ansible . Locate from the results the Ansible service published by Red Hat . Click the blue Install icon located to the right. DEPRECATED VERSION You may ignore the deprecated version of \"Ansible language support\" that was previously published by Tomasz Maciazek. The only official (and installable) Ansible extension is the one published by Red Hat's verified account. Installation of the Ansible extension for VS Code should only take a moment. When it's ready, clear the search bar filter and look for Ansible under the \"Installed\" services under the Extensions panel. Click the mechanical \"cog\" icon (as shown), located on the right side of the Ansible service tile. From the dropdown list of options, click Extension Settings . A settings panel for the Ansible extension will fill the screen. From the top-left corner of the interface, look for a switch to toggle between User and Workspace . Select the Workspace option. Scroll down the list of settings until you locate the fields Ansible > Lightspeed and Ansible > Lightspeed > Suggestions . By default, these will be disabled. Click the checkmark icons to the left of BOTH entries to ENABLE IBM watsonx Code Assistant for Red Hat Ansible Lightspeed Tech Preview features within the VS Code environment. The settings will automatically be applied without needing to confirm or \"save\" manually. Close the Settings tab using the X icon along the top of the taskbar to proceed. Next, you will need to register for an account with GitHub . If you already have an account registered with GitHub, you may skip ahead to Step 9. Register : https://github.com GitHub, Inc. is a platform and cloud-based service for software development and version control using Git , allowing developers to store and manage their code. Like VS Code, it is also owned by Microsoft Corporation. With a registered GitHub account, you are now ready to add the GitHub extension for VS Code to your local machine. This will follow a very similar process to what you performed for installing Ansible. Select the Extensions tab from the left-side of the VS Code interface and search for GitHub Pull Requests . Click the Install icon to add the Extension to your VS Code environment. Integration with VS Code should only take a moment to complete. With the GitHub extension now available within VS Code, you must authorize and link the service to your personal GitHub account. Notice that in the left-hand interface of the VS Code environment, there are now icons (further down from the Extensions tab) for Ansible and GitHub . Click the GitHub tab. In the top-left of the interface, locate and click the blue Sign In button (as shown below). A web browser tab will open, directing you to log in to GitHub using the personal account you registered in Step 8. After authenticating, you will be asked to Authorize Visual-Studio-Code for access to your GitHub account. Do so by clicking the green icon and accepting any prompts. VS Code will ask \"Allow an extension to open this URI?\" to which you should select Open (and toggle \"Don't ask again for this extension\" if you would rather not see future prompts). Once connected to your personal GitHub account, the left side of the VS Code interface will populate with any published code repositories (\"repos\") that are already associated with the account. If you have only registered with GitHub for this lab, this section may appear as empty. To begin experimenting with IBM watsonx Code Assistant for Red Hat Ansible Lightspeed's generative AI capabilities, we first need access to some Ansible Playbooks to generate Tasks with. Playbooks have already been prepared ahead of time for the Ansible Lightspeed Tech Preview, which we will make use of here. A \"pull\" request in GitHub is essentially a request to replicate code from the cloud-hosted repository into the local (VS Code and local machine) environment. To perform a pull request with VS Code, do either of the following actions (depending on your operating system): Windows : With the GitHub Extension tab open in VS Code, press CTRL + Shift + P to open an executable console at the top of VS Code. macOS : With the GitHub Extension tab open in VS Code, press COMMAND + Shift + P to open an executable console at the top of VS Code. The remainder of the instructions are the same for all operating systems. Enter into the terminal prompt the following instruction: git:clone On your keyboard, hit Return/Enter to confirm. Now you must specify the public repository from which to clone the data. Copy and paste the following URL into the terminal prompt: Pull Request URL : https://github.com/craig-br/lightspeed-demos.git On your keyboard, hit Return/Enter to confirm. You will be asked to select (using your local machine's file browser) the destination for where the cloned data will be saved locally. Select a directory (Documents, Desktop, or your preference) and then confirm by clicking Select as Repository Destination to kick off the replication from GitHub to your local machine. A prompt will appear asking Would you like to open the cloned repository, or add it to the current workspace? to which you should click Open . You will be asked Do you trust the authors of the files in this folder? to which you should click Yes, I trust the authors . The replication of Ansible Playbooks to your local machine from GitHub is now complete. You can inspect the contents of the repository using the Explorer tab (the first icon in the left-hand navigator), as shown in the following screenshot. LOCAL REPLICAS The replicated/cloned GitHub files are a local replica, meaning that you may safely edit and change the contents of this folder on your local machine without any impact to the master copy on GitHub. Have fun and experiment! Next, you'll need to activate your Ansible Lightspeed Tech Preview for the Ansible extension inside VS Code. This will follow a similar process to how you previously authorized the GitHub extension. Click the Ansible icon from the left-hand interface of the VS Code environment (look for the large \"A\" icon). A panel will open displaying details about Ansible Lightspeed Login . Click the blue Connect button to launch the authorization tool. A prompt will appear stating that The extension Ansible wants to sign in using Ansible Lightspeed to which you should click Allow . You will then be asked about opening an external website, to which you should reply Open . Your web browser will then load to a page asking you to login in using either Red Hat or GitHub. Click the Log in with GitHub option. OUTDATED PRODUCT NAMES The branding displayed on this page is an outdated name for the offering before reaching General Availability (GA) status. The correct name is IBM watsonx Code Assistant for Red Hat Ansible Lightspeed . Supply your GitHub account name and password (Step 8). If you have already authenticated using the GitHub extension for VS Code, it may bypass that require entirely. After logging in, you will be asked to Authorize Ansible Lightspeed for VS Code to which you should click Authorize . A prompt will attempt inside VS Code asking Allow an extension to open this URI? to which you should click Open . In the bottom-right corner of the VS Code environment, a pop-up will appear with a Welcome back message. Just below this message, along the very bottom-right hand corner of the VS Code environment, a tile labelled Ansible will also have appeared. This indicates that Ansible (and Ansible Lightspeed) are now activated for this particular workspace. PYTHON LIBRARY The VS Code environment also requires that Python 3.9.13 (or higher) be activated as part of the workspace. Look for a Python tile adjacent to the Ansible tile along the bottom-right corner of the VS Code interface. If it is not set, click the tile and select Python 3.9.13 (or higher). Well done. Your VS Code environment is now fully configured and ready for experimentation using generative AI techniques. In the following section, we will review the evaluation criteria for how IBM technical sellers and business partners receive accreditation for completing the Level 3 material.","title":"Setup"},{"location":"Part%201/01-Prerequisites/#prerequisites-and-setup","text":"Before getting started, you must install Visual Studio Code (commonly referred to as VS Code ) on your local machine. VS Code is a code editor built on open source technology, free to use, and published by Microsoft. Distributions are available for every major operating system, including Windows and macOS. Download : https://code.visualstudio.com Select the latest \"stable\" release availabe for your machine's operating system. Follow along with the installer wizard steps and continue after VS Code is successfully running on your local machine. Open the VS Code app and locate the sidebar along the left-side of the interface. Click the Extensions icon as shown to open the marketplace of Microsoft services and open source technologies that can be integrated with VS Code. At the top of the Extensions panel is a search bar. Search here for Ansible . Locate from the results the Ansible service published by Red Hat . Click the blue Install icon located to the right. DEPRECATED VERSION You may ignore the deprecated version of \"Ansible language support\" that was previously published by Tomasz Maciazek. The only official (and installable) Ansible extension is the one published by Red Hat's verified account. Installation of the Ansible extension for VS Code should only take a moment. When it's ready, clear the search bar filter and look for Ansible under the \"Installed\" services under the Extensions panel. Click the mechanical \"cog\" icon (as shown), located on the right side of the Ansible service tile. From the dropdown list of options, click Extension Settings . A settings panel for the Ansible extension will fill the screen. From the top-left corner of the interface, look for a switch to toggle between User and Workspace . Select the Workspace option. Scroll down the list of settings until you locate the fields Ansible > Lightspeed and Ansible > Lightspeed > Suggestions . By default, these will be disabled. Click the checkmark icons to the left of BOTH entries to ENABLE IBM watsonx Code Assistant for Red Hat Ansible Lightspeed Tech Preview features within the VS Code environment. The settings will automatically be applied without needing to confirm or \"save\" manually. Close the Settings tab using the X icon along the top of the taskbar to proceed. Next, you will need to register for an account with GitHub . If you already have an account registered with GitHub, you may skip ahead to Step 9. Register : https://github.com GitHub, Inc. is a platform and cloud-based service for software development and version control using Git , allowing developers to store and manage their code. Like VS Code, it is also owned by Microsoft Corporation. With a registered GitHub account, you are now ready to add the GitHub extension for VS Code to your local machine. This will follow a very similar process to what you performed for installing Ansible. Select the Extensions tab from the left-side of the VS Code interface and search for GitHub Pull Requests . Click the Install icon to add the Extension to your VS Code environment. Integration with VS Code should only take a moment to complete. With the GitHub extension now available within VS Code, you must authorize and link the service to your personal GitHub account. Notice that in the left-hand interface of the VS Code environment, there are now icons (further down from the Extensions tab) for Ansible and GitHub . Click the GitHub tab. In the top-left of the interface, locate and click the blue Sign In button (as shown below). A web browser tab will open, directing you to log in to GitHub using the personal account you registered in Step 8. After authenticating, you will be asked to Authorize Visual-Studio-Code for access to your GitHub account. Do so by clicking the green icon and accepting any prompts. VS Code will ask \"Allow an extension to open this URI?\" to which you should select Open (and toggle \"Don't ask again for this extension\" if you would rather not see future prompts). Once connected to your personal GitHub account, the left side of the VS Code interface will populate with any published code repositories (\"repos\") that are already associated with the account. If you have only registered with GitHub for this lab, this section may appear as empty. To begin experimenting with IBM watsonx Code Assistant for Red Hat Ansible Lightspeed's generative AI capabilities, we first need access to some Ansible Playbooks to generate Tasks with. Playbooks have already been prepared ahead of time for the Ansible Lightspeed Tech Preview, which we will make use of here. A \"pull\" request in GitHub is essentially a request to replicate code from the cloud-hosted repository into the local (VS Code and local machine) environment. To perform a pull request with VS Code, do either of the following actions (depending on your operating system): Windows : With the GitHub Extension tab open in VS Code, press CTRL + Shift + P to open an executable console at the top of VS Code. macOS : With the GitHub Extension tab open in VS Code, press COMMAND + Shift + P to open an executable console at the top of VS Code. The remainder of the instructions are the same for all operating systems. Enter into the terminal prompt the following instruction: git:clone On your keyboard, hit Return/Enter to confirm. Now you must specify the public repository from which to clone the data. Copy and paste the following URL into the terminal prompt: Pull Request URL : https://github.com/craig-br/lightspeed-demos.git On your keyboard, hit Return/Enter to confirm. You will be asked to select (using your local machine's file browser) the destination for where the cloned data will be saved locally. Select a directory (Documents, Desktop, or your preference) and then confirm by clicking Select as Repository Destination to kick off the replication from GitHub to your local machine. A prompt will appear asking Would you like to open the cloned repository, or add it to the current workspace? to which you should click Open . You will be asked Do you trust the authors of the files in this folder? to which you should click Yes, I trust the authors . The replication of Ansible Playbooks to your local machine from GitHub is now complete. You can inspect the contents of the repository using the Explorer tab (the first icon in the left-hand navigator), as shown in the following screenshot. LOCAL REPLICAS The replicated/cloned GitHub files are a local replica, meaning that you may safely edit and change the contents of this folder on your local machine without any impact to the master copy on GitHub. Have fun and experiment! Next, you'll need to activate your Ansible Lightspeed Tech Preview for the Ansible extension inside VS Code. This will follow a similar process to how you previously authorized the GitHub extension. Click the Ansible icon from the left-hand interface of the VS Code environment (look for the large \"A\" icon). A panel will open displaying details about Ansible Lightspeed Login . Click the blue Connect button to launch the authorization tool. A prompt will appear stating that The extension Ansible wants to sign in using Ansible Lightspeed to which you should click Allow . You will then be asked about opening an external website, to which you should reply Open . Your web browser will then load to a page asking you to login in using either Red Hat or GitHub. Click the Log in with GitHub option. OUTDATED PRODUCT NAMES The branding displayed on this page is an outdated name for the offering before reaching General Availability (GA) status. The correct name is IBM watsonx Code Assistant for Red Hat Ansible Lightspeed . Supply your GitHub account name and password (Step 8). If you have already authenticated using the GitHub extension for VS Code, it may bypass that require entirely. After logging in, you will be asked to Authorize Ansible Lightspeed for VS Code to which you should click Authorize . A prompt will attempt inside VS Code asking Allow an extension to open this URI? to which you should click Open . In the bottom-right corner of the VS Code environment, a pop-up will appear with a Welcome back message. Just below this message, along the very bottom-right hand corner of the VS Code environment, a tile labelled Ansible will also have appeared. This indicates that Ansible (and Ansible Lightspeed) are now activated for this particular workspace. PYTHON LIBRARY The VS Code environment also requires that Python 3.9.13 (or higher) be activated as part of the workspace. Look for a Python tile adjacent to the Ansible tile along the bottom-right corner of the VS Code interface. If it is not set, click the tile and select Python 3.9.13 (or higher). Well done. Your VS Code environment is now fully configured and ready for experimentation using generative AI techniques. In the following section, we will review the evaluation criteria for how IBM technical sellers and business partners receive accreditation for completing the Level 3 material.","title":"Prerequisites and Setup"},{"location":"Part%201/02-Evaluation-Criteria/","text":"Evaluation Criteria for IBM and Business Partner Technical Sellers","title":"Evaluation Criteria"},{"location":"Part%201/02-Evaluation-Criteria/#evaluation-criteria-for-ibm-and-business-partner-technical-sellers","text":"","title":"Evaluation Criteria for IBM and Business Partner Technical Sellers"},{"location":"Part%201/03-Generating-Code/","text":"Hands-on: Generating Code with IBM watsonx Code Assistant for Ansible Lightspeed COMMENT & UNCOMMENT LINES OF CODE To quickly uncomment lines of code (remove # characters from the start of a line) using Code VS, simply select the line(s) you wish to comment/uncomment and then press CMD + ? for MacOS or CTRL + ? for Windows. CUSTOM PLAYBOOK #1 - Install an nginx webserver and launch the service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # ANSIBLE PLAYBOOK - Install an nginx webserver and launch the service # AUTHOR \u2014 Deepak C. Shetty (deepakcshetty@in.ibm.com) --- - name : Install nginx hosts : webserver tasks : - name : Add epel-release repo yum : name : epel-release state : present - name : Install nginx yum : name : nginx state : present - name : Insert index page template : src : index.html dest : /usr/share/nginx/html/index.html - name : Start nginx service service : name : nginx state : started TELEMETRY DATA COLLECTION IBM and Red Hat strongly advocate for and put into practice full transparency about what content is collected\u2014 what is known as \" telemetry data \" \u2014to improve the quality of service for AI offerings such as IBM watsonx Code Assistant for Red Hat Ansible Lightspeed. The service automatically collects recommendations, usage telemetry, and Ansible Playbook state through automated events. IBM watsonx Code Assistant for Red Hat Ansible Lightspeed telemetry is used to improve the service over time. Telemetry data that is collected will be shared with Red Hat and IBM. Clients can opt out of this data sharing by disabling (within VS Code) the IBM watsonx Code Assistant for Red Hat Ansible Lightspeed setting in the Ansible VS Code extension. Opting-out of telemetry data sharing does not remove telemetry data already sent to Red Hat and IBM. The telemetry data shared with Red Hat and IBM are selected to be anonymized where applicable and as unobtrusive as possible. Metrics collected include the following: Recommendation accepted/rejected : Captured to indicate whether an AI-generated content suggestion was accepted by the user to include in the actively developed Playbook. Task prompt : The contents of the name property (ie. the natural language prompted by the user) of the task used to request a recommendation from IBM watsonx Code Assistant for Red Hat Ansible Lightspeed. Playbook/Role context : The contents of a Playbook file that exist prior to the task prompt used in the current recommendation. This is used to provide additional context for a recommendation \u2014 in addition to what the natural language task prompt provides. Potentially sensitive information is first removed before collection as telemetry data. Playbook/Role state : This data point is captured at various stages, before and after, the AI-generated recommendation is presented to the user. \"State\" encompasses the contents of an Ansible playbook. This is captured to help the service understand what components of a recommendation were valuable (or not) once a recommendation is provided. Anonymized document Uniform Resource Identifier (URI) : Used to determine the kind of Ansible document (Playbook, Role, or so on). Request and response timestamps : Captured in UTC format. Anonymized user ID : Captured at (~/.redhat/anonymousid). Anonymized suggestion ID : Automatically generated session ID associated with service request. Telemetry data may be kept indefinitely for product improvement. Users are not able to request that their telemetry be purged. Clients may disable the Ansible Lightspeed service to prevent further telemetry from being sent to the service. Red Hat and IBM do not claim any copyright or other intellectual property rights to the suggestions generated by IBM watsonx Code Assistant for Red Hat Ansible Lightspeed. Additional details can be located within the Terms of Service for the offering: https://docs.ai.ansible.redhat.com/tos/ lightspeed-demos-main/playbooks/cloud/aws/demo_provision_ec2_instance.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- - name : EC2 Cloud Operations hosts : localhost connection : local gather_facts : false module_defaults : group/aws : region : us-east-1 # vars: # ec2_instance: # name: lightspeed-instance-01 # key_name: lightspeed-keypair # image_id: ami-016eb5d644c333ccb # RHEL 9 us-east-1 # tags: # function: lightspeed-demo # security_group: secgroup-lightspeed tasks : # TASK 1 # # 1a. Uncomment task description below and generate a task suggestion. # # Note - Best practices: The suggestion used the Fully Qualified Collection name. # # Note - Context: Ansible Lightspeed used the Playbook name \"EC2 Cloud Operations\" to use the correct \"amazon.aws.ec2_vpc_subnet_info\" module. - name : Gather subnet info tag:Name subnet-lightspeed # TASK 2 # # 2a. Uncomment task description below and generate a task suggestion. # # Note - Context: The suggestion included the previous task's registered variable in the suggestion. # # Note - Accuracy: The suggestion provides the correct key value from the previously task's registered variable. - name : Create vpc_subnet_id var # TASK 3 # # 3a. Uncomment task description \"Provision t3.micro instance\" below and generate a task suggestion. # # Note - Efficiency: The suggestion provides practical variable examples to improve efficiency. - name : Provision t3.micro instance # # 3b. Remove the above task and suggestion. # # Uncomment 2nd task description \"Provision t3.micro instance using ec2_instance var\". # # Generate an updated suggestion. # # Note - Context: The updated suggestion includes the \"ec2_instance variable fields in the suggestion\" - name : Provision t3.micro instance using ec2_instance var lightspeed-demos-main/playbooks/infra/install_cockpit/demo_install_cockpit.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 --- - name : Install and configure Cockpit hosts : rhel become : true # module_defaults: # ansible.builtin.service: # enabled: true # state: started tasks : # TASK 1 # # 1a. Uncomment task description below and generate a task suggestion. # # Note - The suggestion included Ansible best practices by using Fully Qualified Collection name. - name : Install cockpit package # TASK 2 # # 2a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used \"ansible.builtin.template\" module based on the \".j2\" file extension. # # Note - The suggestion set the file permissions (\"0644\"), owner, and group based on Ansible best practices. - name : Copy cockpit.conf.j2 to /etc/cockpit # TASK 3 # # 3a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used the generic \"Start and enable service\" prompt # # and full Playbook context to infer the recommendation should start the \"cockpit\" service. # # 3b. Uncomment the \"module_defaults\" section at the top of the Playbook. # # 3c. Clear current task suggestion and request updated suggestion. # # Note - Ansible Lightspeed used the full Playbook context and evaluated the \"module_defaults\" # # when generating a suggestion. # # The updated suggestion no longer includes \"enabled:\" and \"state:\" arguments. - name : Start and enable service # TASK 4 # # 4a. Uncomment task description below and generate an Ansible Playbook task suggestion. # # 4b. Save the Playbook. - name : Wait 15 seconds port 9090 Content source matching and attribution attempts to match code recommendations to Ansible Galaxy sources (projects and contributors) for data provenance. lightspeed-demos-main/playbooks/infra/install_pgsql_and_pgadmin/demo_install_pgsql.yml 1 2 3 4 5 6 7 8 9 10 11 --- - name : Configure Database servers hosts : databases become : true tasks : - name : Install postgresql-server - name : Run postgresql setup command - name : Start and enable postgresql service A powerful capable within IBM watsonx Code Assistant for Red Hat Ansible Lightspeed is code explainability , which attempts to match AI-generated code suggestions to training data and sources that were utilized in generating the suggested Task code. For the Technical Preview (pre-GA) release of the service, the extent of these capabilities are fairly rudimentary \u2014 identifying Ansible Galaxy packages and contributors where available for each line of AI-generated code. PROJECTED CAPABILITIES In the future, it is anticipated that clients will be able to supply Ansible Lightspeed with a Playbook and ask \"where did this generated code come from and who was its author?\" The service may potentially gain the ability to \"comment\" and document mark-up lines (or blocks) of code, explaining to users in plain English what functions of code are performing \u2014 work traditionally left to the developer or author, but an area often neglected or lacking in many organization's code bases. These code attribution suggestions are created using a k-NN ( K-Nearest Neighbors ) algorithm that examines Ansible Galaxy and training data repositories in search of the nearest related content to the AI-generated code suggestions. lightspeed-demos-main/playbooks/infra/install_pgsql_and_pgadmin/demo_pgadmin_podman.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 --- - name : Configure pgadmin container hosts : webservers become : true vars : pgadmin_service_name : app-pgadmin pgadmin_container : name : pgadmin restart : false image : docker.io/dpage/pgadmin4:7.5 state : started env : PGADMIN_DEFAULT_EMAIL : student@example.com PGADMIN_DEFAULT_PASSWORD : learn_ansible generate_systemd : path : /etc/systemd/system/ container_prefix : app restart_policy : always ports : - 8083:80 network : bridge tasks : - name : Run podman container using pgadmin_container var - name : Start, enable and reload daemon for {{ pgadmin_service_name }}","title":"Generating Code"},{"location":"Part%201/03-Generating-Code/#_1","text":"","title":""},{"location":"Part%201/03-Generating-Code/#hands-on-generating-code-with-ibm-watsonx-code-assistant-for-ansible-lightspeed","text":"COMMENT & UNCOMMENT LINES OF CODE To quickly uncomment lines of code (remove # characters from the start of a line) using Code VS, simply select the line(s) you wish to comment/uncomment and then press CMD + ? for MacOS or CTRL + ? for Windows. CUSTOM PLAYBOOK #1 - Install an nginx webserver and launch the service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # ANSIBLE PLAYBOOK - Install an nginx webserver and launch the service # AUTHOR \u2014 Deepak C. Shetty (deepakcshetty@in.ibm.com) --- - name : Install nginx hosts : webserver tasks : - name : Add epel-release repo yum : name : epel-release state : present - name : Install nginx yum : name : nginx state : present - name : Insert index page template : src : index.html dest : /usr/share/nginx/html/index.html - name : Start nginx service service : name : nginx state : started TELEMETRY DATA COLLECTION IBM and Red Hat strongly advocate for and put into practice full transparency about what content is collected\u2014 what is known as \" telemetry data \" \u2014to improve the quality of service for AI offerings such as IBM watsonx Code Assistant for Red Hat Ansible Lightspeed. The service automatically collects recommendations, usage telemetry, and Ansible Playbook state through automated events. IBM watsonx Code Assistant for Red Hat Ansible Lightspeed telemetry is used to improve the service over time. Telemetry data that is collected will be shared with Red Hat and IBM. Clients can opt out of this data sharing by disabling (within VS Code) the IBM watsonx Code Assistant for Red Hat Ansible Lightspeed setting in the Ansible VS Code extension. Opting-out of telemetry data sharing does not remove telemetry data already sent to Red Hat and IBM. The telemetry data shared with Red Hat and IBM are selected to be anonymized where applicable and as unobtrusive as possible. Metrics collected include the following: Recommendation accepted/rejected : Captured to indicate whether an AI-generated content suggestion was accepted by the user to include in the actively developed Playbook. Task prompt : The contents of the name property (ie. the natural language prompted by the user) of the task used to request a recommendation from IBM watsonx Code Assistant for Red Hat Ansible Lightspeed. Playbook/Role context : The contents of a Playbook file that exist prior to the task prompt used in the current recommendation. This is used to provide additional context for a recommendation \u2014 in addition to what the natural language task prompt provides. Potentially sensitive information is first removed before collection as telemetry data. Playbook/Role state : This data point is captured at various stages, before and after, the AI-generated recommendation is presented to the user. \"State\" encompasses the contents of an Ansible playbook. This is captured to help the service understand what components of a recommendation were valuable (or not) once a recommendation is provided. Anonymized document Uniform Resource Identifier (URI) : Used to determine the kind of Ansible document (Playbook, Role, or so on). Request and response timestamps : Captured in UTC format. Anonymized user ID : Captured at (~/.redhat/anonymousid). Anonymized suggestion ID : Automatically generated session ID associated with service request. Telemetry data may be kept indefinitely for product improvement. Users are not able to request that their telemetry be purged. Clients may disable the Ansible Lightspeed service to prevent further telemetry from being sent to the service. Red Hat and IBM do not claim any copyright or other intellectual property rights to the suggestions generated by IBM watsonx Code Assistant for Red Hat Ansible Lightspeed. Additional details can be located within the Terms of Service for the offering: https://docs.ai.ansible.redhat.com/tos/ lightspeed-demos-main/playbooks/cloud/aws/demo_provision_ec2_instance.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- - name : EC2 Cloud Operations hosts : localhost connection : local gather_facts : false module_defaults : group/aws : region : us-east-1 # vars: # ec2_instance: # name: lightspeed-instance-01 # key_name: lightspeed-keypair # image_id: ami-016eb5d644c333ccb # RHEL 9 us-east-1 # tags: # function: lightspeed-demo # security_group: secgroup-lightspeed tasks : # TASK 1 # # 1a. Uncomment task description below and generate a task suggestion. # # Note - Best practices: The suggestion used the Fully Qualified Collection name. # # Note - Context: Ansible Lightspeed used the Playbook name \"EC2 Cloud Operations\" to use the correct \"amazon.aws.ec2_vpc_subnet_info\" module. - name : Gather subnet info tag:Name subnet-lightspeed # TASK 2 # # 2a. Uncomment task description below and generate a task suggestion. # # Note - Context: The suggestion included the previous task's registered variable in the suggestion. # # Note - Accuracy: The suggestion provides the correct key value from the previously task's registered variable. - name : Create vpc_subnet_id var # TASK 3 # # 3a. Uncomment task description \"Provision t3.micro instance\" below and generate a task suggestion. # # Note - Efficiency: The suggestion provides practical variable examples to improve efficiency. - name : Provision t3.micro instance # # 3b. Remove the above task and suggestion. # # Uncomment 2nd task description \"Provision t3.micro instance using ec2_instance var\". # # Generate an updated suggestion. # # Note - Context: The updated suggestion includes the \"ec2_instance variable fields in the suggestion\" - name : Provision t3.micro instance using ec2_instance var lightspeed-demos-main/playbooks/infra/install_cockpit/demo_install_cockpit.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 --- - name : Install and configure Cockpit hosts : rhel become : true # module_defaults: # ansible.builtin.service: # enabled: true # state: started tasks : # TASK 1 # # 1a. Uncomment task description below and generate a task suggestion. # # Note - The suggestion included Ansible best practices by using Fully Qualified Collection name. - name : Install cockpit package # TASK 2 # # 2a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used \"ansible.builtin.template\" module based on the \".j2\" file extension. # # Note - The suggestion set the file permissions (\"0644\"), owner, and group based on Ansible best practices. - name : Copy cockpit.conf.j2 to /etc/cockpit # TASK 3 # # 3a. Uncomment task description below and generate a task suggestion. # # Note - Ansible Lightspeed used the generic \"Start and enable service\" prompt # # and full Playbook context to infer the recommendation should start the \"cockpit\" service. # # 3b. Uncomment the \"module_defaults\" section at the top of the Playbook. # # 3c. Clear current task suggestion and request updated suggestion. # # Note - Ansible Lightspeed used the full Playbook context and evaluated the \"module_defaults\" # # when generating a suggestion. # # The updated suggestion no longer includes \"enabled:\" and \"state:\" arguments. - name : Start and enable service # TASK 4 # # 4a. Uncomment task description below and generate an Ansible Playbook task suggestion. # # 4b. Save the Playbook. - name : Wait 15 seconds port 9090","title":"Hands-on: Generating Code with IBM watsonx Code Assistant for Ansible Lightspeed"},{"location":"Part%201/03-Generating-Code/#_2","text":"","title":""},{"location":"Part%201/03-Generating-Code/#content-source-matching-and-attribution-attempts-to-match-code-recommendations-to-ansible-galaxy-sources-projects-and-contributors-for-data-provenance","text":"lightspeed-demos-main/playbooks/infra/install_pgsql_and_pgadmin/demo_install_pgsql.yml 1 2 3 4 5 6 7 8 9 10 11 --- - name : Configure Database servers hosts : databases become : true tasks : - name : Install postgresql-server - name : Run postgresql setup command - name : Start and enable postgresql service A powerful capable within IBM watsonx Code Assistant for Red Hat Ansible Lightspeed is code explainability , which attempts to match AI-generated code suggestions to training data and sources that were utilized in generating the suggested Task code. For the Technical Preview (pre-GA) release of the service, the extent of these capabilities are fairly rudimentary \u2014 identifying Ansible Galaxy packages and contributors where available for each line of AI-generated code. PROJECTED CAPABILITIES In the future, it is anticipated that clients will be able to supply Ansible Lightspeed with a Playbook and ask \"where did this generated code come from and who was its author?\" The service may potentially gain the ability to \"comment\" and document mark-up lines (or blocks) of code, explaining to users in plain English what functions of code are performing \u2014 work traditionally left to the developer or author, but an area often neglected or lacking in many organization's code bases. These code attribution suggestions are created using a k-NN ( K-Nearest Neighbors ) algorithm that examines Ansible Galaxy and training data repositories in search of the nearest related content to the AI-generated code suggestions. lightspeed-demos-main/playbooks/infra/install_pgsql_and_pgadmin/demo_pgadmin_podman.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 --- - name : Configure pgadmin container hosts : webservers become : true vars : pgadmin_service_name : app-pgadmin pgadmin_container : name : pgadmin restart : false image : docker.io/dpage/pgadmin4:7.5 state : started env : PGADMIN_DEFAULT_EMAIL : student@example.com PGADMIN_DEFAULT_PASSWORD : learn_ansible generate_systemd : path : /etc/systemd/system/ container_prefix : app restart_policy : always ports : - 8083:80 network : bridge tasks : - name : Run podman container using pgadmin_container var - name : Start, enable and reload daemon for {{ pgadmin_service_name }}","title":"Content source matching and attribution attempts to match code recommendations to Ansible Galaxy sources (projects and contributors) for data provenance."},{"location":"Part%201/04-Custom-Playbooks/","text":"Hands-on: Customizing Ansible Tasks with IBM watsonx Code Assistant for Ansible Lightspeed In the following module, you will begin experimentating with customized Ansible Playbooks and testing how changes to Ansible Task natural language descriptions impacts the recommended code produced by Generative AI. A sample Playbook is supplied for you below. CUSTOM PLAYBOOK #1 - Invoke 2 modules to automatically update 2 types of servers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # ANSIBLE PLAYBOOK \u2014 Invoke 2 modules to automatically update 2 types of servers. --- # Task 1 - name : Update web servers hosts : webservers become : true tasks : - name : Ensure apache is at the latest version ansible.builtin.yum : name : httpd state : latest - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" # Task 2 - name : Update db servers hosts : databases become : true tasks : - name : Ensure postgresql is at the latest version ansible.builtin.yum : name : postgresql state : latest - name : Ensure that postgresql is started ansible.builtin.service : name : postgresql state : started Copy the contents above to clipboard using the button in the top-right corner of the code block. Return to your VS Code environment. In the top-left corner of the interface, with your Ansible Lightspeed folder selected, click the New File... button. Name the file to a description of your choosing, ending with .yml as the filetype. Set it to CustomPlaybook.yml , for example. Paste the clipboard contents into the YAML file and follow along with the suggestions below. Precision is key for disambiguation of natural language prompts. Custom Playbook #1 (above) contains 2 sets of tasks: Task 1 ( Lines 5-20 ) checks whether or not web server software is up to date and runs the update if necessary. Task 2 ( Lines 22-36 ) checks whether or not database server software is up to date and runs the update if necessary. Consider the following Task ( Line 16 ), which prompts Ansible to create (\"write\") a configuration file for an Apache webserver: - name : Write the apache config file Two tabs are presented below. The first AI-Generated Code tab shows the output from running Ansible Lightspeed's generative AI capabilities on an unmodified version of this Playbook. The second Solution Code tab shows the expected Task code that was written (and commented out) by a human to perform the same task. In theory, the suggested code should be as good\u2014 or even superior to \u2014the manually-written solution code. Let's examine that theory in practice: AI-Generated Code Solution Code 1 2 3 4 - name : Write the apache config file ansible.builtin.copy : content : \"{{ _content_ }}\" dest : /etc/httpd/conf.d/000-default.conf 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" As you can see, the AI-Generated Code in the first tab misses the mark in a few areas. In fact, it appears to have misunderstood the task in quite a few ways. There is no corresponding value to _content_ that can be located within the Ansible Playbook, at least not any which you (the author) have defined ahead of time. Such a result is unexpected and does not match the intention of writing an Apache configuration file to standards/best practices. Similarly, the destination has been inferred differently and the access permissions that should have been applied to the config file (again - according to Red Hat best practices) is missing altogether. WHY THE UNEXPECTED RESULTS? Something has gone wrong \u2014 is Ansible Lightspeed at fault? The root cause of the error, in fact, is human . The precision with which the Playbook author describes the automation Task in natural language will determine the accuracy and effectiveness of Ansible Lightspeed's generated code suggestions. In general, the more ambiguous the Task description given, the greater the likelihood that Ansible Lightspeed will misinterpret the author's intent and suggest unwanted Ansible automation jobs. To help disambiguate our intention, we as Playbook authors can use more precise natural language terms and descriptions. Let's slightly modify the Task description. Try revising the Task -name: ... description to the following, hit Return , and accept the generated results with the TAB key: - name : Write the apache config file with mode 0644 Compare how the AI-generated code suggestions match (or don't) the expected code solution: AI-Generated Code Solution Code 1 2 3 4 5 - name : Write the apache config file with mode 0644 ansible.builtin.copy : dest : /etc/httpd/conf/httpd.conf content : \"Alias / /var/www/html/\\n\\n<Directory /var/www/html/\\\">\\n <Files>\\n\\\\ &2\\n &1\\n </Files>\\n </Directory>\\n</Dispatcher>\\n\" mode : '0644' 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" The output is closer to the expected on the latest iteration, but still out of alignment in some aspects. The AI-generated code suggestions now correctly include the 06444 read/write/execute permissions that were requested. However, an incorrect destination was suggested (instead of a source) and the content field is a mismatch to the request entirely. One other observation is that the suggested code is invoking the ansible.builtin.copy package, instead of the expected ansible.builtin.template package. What happens if we adjust the Task's natural language description to explicitly invoke the .template package? Adjust the Task description to the following and then re-generate the code block using Ansible Lightspeed: - name : Write the apache config file using mode 0644 from template Once again, compare the AI-generated code suggestions to the expected code solution: AI-Generated Code Solution Code 1 2 3 4 - name : Write the apache config file using mode 0644 from template ansible.builtin.copy : src : /home/ano-user/automation/ansible/templates/httpd.j2 dest : /etc/httpd/conf/httpd.conf 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" Annoyingly, the request to generate \" from template \" appears to have been misinterpreted once again. Looking at the syntax of the natural language description gives us some clues as to why Line 1 of AI-Generated Code , if you consider it the same way as a natural language processor would, is essentially a string of individual tokens. Every word, number, or character separated by a whitespace (a \"tokenizer\") is a \"token.\" In our example, mode and 0644 are separate tokens \u2014 and we expect that the natural language capabilities will understand the implied link between these two. But we can make this linkage more explicit by being more exact with how we describe these two elements in the next iteration. Next time, we'll write them as a single token with a very explicit linkage by using mode='0644' and compare the results. Line 2 of AI-Generated Code once again failed to invoke the expected ansible.builtin.template package. At this stage, you may speculate that the reason for this is the ambiguity created by the separate mode and 0644 tokens that were described previously. We will observe on the next generative iteration whether disambiguation of those two tokens helps clarify the meaning of the from template tokens. The mode: , src: , and dest: fields from Lines 3-5 of Solution Code are incorrect or missing. For a third time, let's adjust the natural language description of the Task to the following and then re-generate the code block using Ansible Lightspeed: - name : Write the apache config file using mode='0644' from template Once again, compare the AI-generated code suggestions to the expected code solution: AI-Generated Code Solution Code 1 2 3 4 5 6 7 - name : Write the apache config file using mode='0644' from template ansible.builtin.template : src : httpd.j2 dest : /etc/httpd/conf/httpd.conf owner : root group : root mode : '0644' 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" The resulting code suggestions from a simple tweak of mode 0644 to mode='0644' to disambiguate the purpose of the Task is far better than previous iterations. Not only was the mode: variable correctly set, but the generative AI code correctly picked up on the intention to invoke the Ansible builtin.template package. The iterative process we have gone through with this example can be viewed in two different ways. On the one hand, it shows the sensitivity of these generative AI models to even the most nuanced change in natural language prompts \u2014 for good or bad. Generative AI can produce tremendous work and that output is further guided along by best practices built-in from Red Hat and IBM. But in the end, the AI can only infer user intent from the natural language descriptions supplied to it. The less vague our descriptions of intent, the more likely it will correctly generate code that mirrors that intent (and conversely \u2014 the less precise we are, the more likely it will misinterpret and miss the mark). Precision is key to the disambiguation of natural language prompts. Human feedback and humans-in-the-loop are essential to these formative stages of Generative AI. As offerings like IBM watsonx Code Assistant for Red Hat Ansible Lightspeed achieve general availability (GA) status and exit the Tech Preview phase, the natural language processing capabilities of the service will continue to refine and improve. Additional packages, functions, and training data from Ansible Galaxy (as well as other sources) are continuously being added to the service's Foundation Models, which will in turn continually improve the AI-generated code recommendations made to users. Feedback can be directly supplied via the Ansible Lightspeed Code VS extension and additional ways to support model training are documented online . LEVEL 3 ACCREDITATION IBMers should take the time now to prepare for and record their Stand & Deliver presentations for Level 3 accreditation. Be sure to follow the evaluation criteria that is outlined in this documentation. Business Partners should follow the learning plan links on IBM Training to complete a multiple-choice examination for Level 3 accreditation. That concludes the hands-on components to this Level 3 course, but your learning and experimentation doesn't need to end here. Continue to experiment with generating Tasks for Ansible Playbooks. If you have worked extensively with Ansible previously, try recreating Tasks you've written previously from old Playbooks using generative AI (just be careful not to use confidential/sensitive information as part of those tests) \u2013 see if you can spot the differences or improvements made from the code recommendations. Feel free to reach out to the authors of this coursework if you have suggestions of Tasks or code generation techniques that you'd like to see included in future iterations of this hands-on training.","title":"Custom Playbooks"},{"location":"Part%201/04-Custom-Playbooks/#hands-on-customizing-ansible-tasks-with-ibm-watsonx-code-assistant-for-ansible-lightspeed","text":"In the following module, you will begin experimentating with customized Ansible Playbooks and testing how changes to Ansible Task natural language descriptions impacts the recommended code produced by Generative AI. A sample Playbook is supplied for you below. CUSTOM PLAYBOOK #1 - Invoke 2 modules to automatically update 2 types of servers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # ANSIBLE PLAYBOOK \u2014 Invoke 2 modules to automatically update 2 types of servers. --- # Task 1 - name : Update web servers hosts : webservers become : true tasks : - name : Ensure apache is at the latest version ansible.builtin.yum : name : httpd state : latest - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" # Task 2 - name : Update db servers hosts : databases become : true tasks : - name : Ensure postgresql is at the latest version ansible.builtin.yum : name : postgresql state : latest - name : Ensure that postgresql is started ansible.builtin.service : name : postgresql state : started Copy the contents above to clipboard using the button in the top-right corner of the code block. Return to your VS Code environment. In the top-left corner of the interface, with your Ansible Lightspeed folder selected, click the New File... button. Name the file to a description of your choosing, ending with .yml as the filetype. Set it to CustomPlaybook.yml , for example. Paste the clipboard contents into the YAML file and follow along with the suggestions below.","title":"Hands-on: Customizing Ansible Tasks with IBM watsonx Code Assistant for Ansible Lightspeed"},{"location":"Part%201/04-Custom-Playbooks/#_1","text":"","title":""},{"location":"Part%201/04-Custom-Playbooks/#precision-is-key-for-disambiguation-of-natural-language-prompts","text":"Custom Playbook #1 (above) contains 2 sets of tasks: Task 1 ( Lines 5-20 ) checks whether or not web server software is up to date and runs the update if necessary. Task 2 ( Lines 22-36 ) checks whether or not database server software is up to date and runs the update if necessary. Consider the following Task ( Line 16 ), which prompts Ansible to create (\"write\") a configuration file for an Apache webserver: - name : Write the apache config file Two tabs are presented below. The first AI-Generated Code tab shows the output from running Ansible Lightspeed's generative AI capabilities on an unmodified version of this Playbook. The second Solution Code tab shows the expected Task code that was written (and commented out) by a human to perform the same task. In theory, the suggested code should be as good\u2014 or even superior to \u2014the manually-written solution code. Let's examine that theory in practice: AI-Generated Code Solution Code 1 2 3 4 - name : Write the apache config file ansible.builtin.copy : content : \"{{ _content_ }}\" dest : /etc/httpd/conf.d/000-default.conf 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" As you can see, the AI-Generated Code in the first tab misses the mark in a few areas. In fact, it appears to have misunderstood the task in quite a few ways. There is no corresponding value to _content_ that can be located within the Ansible Playbook, at least not any which you (the author) have defined ahead of time. Such a result is unexpected and does not match the intention of writing an Apache configuration file to standards/best practices. Similarly, the destination has been inferred differently and the access permissions that should have been applied to the config file (again - according to Red Hat best practices) is missing altogether. WHY THE UNEXPECTED RESULTS? Something has gone wrong \u2014 is Ansible Lightspeed at fault? The root cause of the error, in fact, is human . The precision with which the Playbook author describes the automation Task in natural language will determine the accuracy and effectiveness of Ansible Lightspeed's generated code suggestions. In general, the more ambiguous the Task description given, the greater the likelihood that Ansible Lightspeed will misinterpret the author's intent and suggest unwanted Ansible automation jobs. To help disambiguate our intention, we as Playbook authors can use more precise natural language terms and descriptions. Let's slightly modify the Task description. Try revising the Task -name: ... description to the following, hit Return , and accept the generated results with the TAB key: - name : Write the apache config file with mode 0644 Compare how the AI-generated code suggestions match (or don't) the expected code solution: AI-Generated Code Solution Code 1 2 3 4 5 - name : Write the apache config file with mode 0644 ansible.builtin.copy : dest : /etc/httpd/conf/httpd.conf content : \"Alias / /var/www/html/\\n\\n<Directory /var/www/html/\\\">\\n <Files>\\n\\\\ &2\\n &1\\n </Files>\\n </Directory>\\n</Dispatcher>\\n\" mode : '0644' 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" The output is closer to the expected on the latest iteration, but still out of alignment in some aspects. The AI-generated code suggestions now correctly include the 06444 read/write/execute permissions that were requested. However, an incorrect destination was suggested (instead of a source) and the content field is a mismatch to the request entirely. One other observation is that the suggested code is invoking the ansible.builtin.copy package, instead of the expected ansible.builtin.template package. What happens if we adjust the Task's natural language description to explicitly invoke the .template package? Adjust the Task description to the following and then re-generate the code block using Ansible Lightspeed: - name : Write the apache config file using mode 0644 from template Once again, compare the AI-generated code suggestions to the expected code solution: AI-Generated Code Solution Code 1 2 3 4 - name : Write the apache config file using mode 0644 from template ansible.builtin.copy : src : /home/ano-user/automation/ansible/templates/httpd.j2 dest : /etc/httpd/conf/httpd.conf 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" Annoyingly, the request to generate \" from template \" appears to have been misinterpreted once again. Looking at the syntax of the natural language description gives us some clues as to why Line 1 of AI-Generated Code , if you consider it the same way as a natural language processor would, is essentially a string of individual tokens. Every word, number, or character separated by a whitespace (a \"tokenizer\") is a \"token.\" In our example, mode and 0644 are separate tokens \u2014 and we expect that the natural language capabilities will understand the implied link between these two. But we can make this linkage more explicit by being more exact with how we describe these two elements in the next iteration. Next time, we'll write them as a single token with a very explicit linkage by using mode='0644' and compare the results. Line 2 of AI-Generated Code once again failed to invoke the expected ansible.builtin.template package. At this stage, you may speculate that the reason for this is the ambiguity created by the separate mode and 0644 tokens that were described previously. We will observe on the next generative iteration whether disambiguation of those two tokens helps clarify the meaning of the from template tokens. The mode: , src: , and dest: fields from Lines 3-5 of Solution Code are incorrect or missing. For a third time, let's adjust the natural language description of the Task to the following and then re-generate the code block using Ansible Lightspeed: - name : Write the apache config file using mode='0644' from template Once again, compare the AI-generated code suggestions to the expected code solution: AI-Generated Code Solution Code 1 2 3 4 5 6 7 - name : Write the apache config file using mode='0644' from template ansible.builtin.template : src : httpd.j2 dest : /etc/httpd/conf/httpd.conf owner : root group : root mode : '0644' 1 2 3 4 5 - name : Write the apache config file ansible.builtin.template : src : /srv/httpd.j2 dest : /etc/httpd.config mode : \"0644\" The resulting code suggestions from a simple tweak of mode 0644 to mode='0644' to disambiguate the purpose of the Task is far better than previous iterations. Not only was the mode: variable correctly set, but the generative AI code correctly picked up on the intention to invoke the Ansible builtin.template package. The iterative process we have gone through with this example can be viewed in two different ways. On the one hand, it shows the sensitivity of these generative AI models to even the most nuanced change in natural language prompts \u2014 for good or bad. Generative AI can produce tremendous work and that output is further guided along by best practices built-in from Red Hat and IBM. But in the end, the AI can only infer user intent from the natural language descriptions supplied to it. The less vague our descriptions of intent, the more likely it will correctly generate code that mirrors that intent (and conversely \u2014 the less precise we are, the more likely it will misinterpret and miss the mark). Precision is key to the disambiguation of natural language prompts. Human feedback and humans-in-the-loop are essential to these formative stages of Generative AI. As offerings like IBM watsonx Code Assistant for Red Hat Ansible Lightspeed achieve general availability (GA) status and exit the Tech Preview phase, the natural language processing capabilities of the service will continue to refine and improve. Additional packages, functions, and training data from Ansible Galaxy (as well as other sources) are continuously being added to the service's Foundation Models, which will in turn continually improve the AI-generated code recommendations made to users. Feedback can be directly supplied via the Ansible Lightspeed Code VS extension and additional ways to support model training are documented online . LEVEL 3 ACCREDITATION IBMers should take the time now to prepare for and record their Stand & Deliver presentations for Level 3 accreditation. Be sure to follow the evaluation criteria that is outlined in this documentation. Business Partners should follow the learning plan links on IBM Training to complete a multiple-choice examination for Level 3 accreditation. That concludes the hands-on components to this Level 3 course, but your learning and experimentation doesn't need to end here. Continue to experiment with generating Tasks for Ansible Playbooks. If you have worked extensively with Ansible previously, try recreating Tasks you've written previously from old Playbooks using generative AI (just be careful not to use confidential/sensitive information as part of those tests) \u2013 see if you can spot the differences or improvements made from the code recommendations. Feel free to reach out to the authors of this coursework if you have suggestions of Tasks or code generation techniques that you'd like to see included in future iterations of this hands-on training.","title":"Precision is key for disambiguation of natural language prompts."},{"location":"Part%201/05-Advanced-Topics-EC2-WordPress/","text":"Macro Rendering Error UndefinedError : 'wordpress_vpc' is undefined Traceback (most recent call last): File \"/opt/homebrew/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 480, in render return md_template.render(**page_variables) File \"/opt/homebrew/lib/python3.9/site-packages/jinja2/environment.py\", line 1301, in render self.environment.handle_exception() File \"/opt/homebrew/lib/python3.9/site-packages/jinja2/environment.py\", line 936, in handle_exception raise rewrite_traceback_stack(source=source) File \"<template>\", line 39, in top-level template code File \"/opt/homebrew/lib/python3.9/site-packages/jinja2/environment.py\", line 485, in getattr return getattr(obj, attribute) jinja2.exceptions.UndefinedError: 'wordpress_vpc' is undefined","title":"Advanced - Deploying an AWS EC2 VPC and WordPress Application"},{"location":"Part%201/05-Advanced-Topics-EC2-WordPress/#macro-rendering-error","text":"UndefinedError : 'wordpress_vpc' is undefined Traceback (most recent call last): File \"/opt/homebrew/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 480, in render return md_template.render(**page_variables) File \"/opt/homebrew/lib/python3.9/site-packages/jinja2/environment.py\", line 1301, in render self.environment.handle_exception() File \"/opt/homebrew/lib/python3.9/site-packages/jinja2/environment.py\", line 936, in handle_exception raise rewrite_traceback_stack(source=source) File \"<template>\", line 39, in top-level template code File \"/opt/homebrew/lib/python3.9/site-packages/jinja2/environment.py\", line 485, in getattr return getattr(obj, attribute) jinja2.exceptions.UndefinedError: 'wordpress_vpc' is undefined","title":"Macro Rendering Error"}]}